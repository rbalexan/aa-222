\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{svg}

\usepackage{booktabs} %@{}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage[per-mode=symbol,detect-all]{siunitx}
\usepackage{hyperref}
\usepackage{cleveref} %\Cref{} vs. \cref{}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{mathabx} % for \bigtimes

\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\argmax}{arg~max}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
	
	
	\title{\LARGE \textbf{Active Learning for Efficiently Constructing Surrogate Models}}
	
	
	\author{\IEEEauthorblockN{  Ross Alexander}
		\IEEEauthorblockA{\textit{  Department of Aeronautics and Astronautics} \\
			\textit{                    Stanford University} \\
			Stanford, CA 94305 \\
			rbalexan@stanford.edu}} % or ORCID
	
	
	\maketitle
	
	\begin{abstract}
		The design of experiments is the task of designing a one-shot sampling scheme over the input space that will efficiently capture variation in the output space. While a static, dense sampling scheme is often easy to design and execute, for many experiments, the cost of evaluating such a sample set can be expensive, time-consuming, or even impossible. In order to improve sample efficiency, we examine the sequential design of experiments, where the task consists of alternating between constructing a surrogate model using the current set of samples and identifying the next sample location utilizing information from the surrogate model. We compare three traditional sampling techniques against two active learning techniques on both Gaussian process (GP) and artificial neural network (NN) surrogate models across a variety of one-dimensional test functions. We find that GP surrogates outperform NN surrogates in sequential design of experiments tasks where the number of samples is limited and when prior information about the latent function can be incorporated into the GP.
	\end{abstract}
	
	\section{Introduction}
	\label{sec:introduction}
	
	% What is the problem? 
	The design of experiments (DOE) is a task that focuses on constructing a sampling scheme that, when sampled, explains the variation of an experiment's output space with respect to its input space. Typically, designers choose to sample according to a dense grid, since it is easy to implement by specifying the upper and lower bounds and resolution of the grid. However, in many technical experimental domains (e.g. hypersonic wind tunnel tests, biological cell assays, particle collider tests, etc.), experiments can be expensive, time-consuming, or impossible to evaluate. Thus, traditional design of experiments utilizing dense sampling schemes is often intractable. In order to make these experiments feasible, we must use a more sample-efficient method to generate our sample set. 
	
	% Why is it interesting and important?
	Sequential design of experiments generalizes traditional design of experiments where we repeatedly construct a surrogate model using the current set of samples and then identify the next sample location using evaluations of the current surrogate model. The proposed sample location is sampled in the experiment or simulation and then added to the set of evaluated samples. This process repeats until the model is assumed accurate or until a maximum number of samples have been evaluated. This process is depicted in \Cref{fig:sdoe_diagram}
	
	\begin{figure}[htbp]
	    \centering
	    \includesvg[width=0.51\linewidth]{../../src/plots/model_0_0.svg} \hspace*{-1.5em}
	    \includesvg[width=0.51\linewidth]{../../src/plots/model_0_1.svg}
	    \includesvg[width=0.51\linewidth]{../../src/plots/model_0_2.svg} \hspace*{-1.5em}
	    \includesvg[width=0.51\linewidth]{../../src/plots/model_0_3.svg}
	    \includesvg[width=0.51\linewidth]{../../src/plots/model_0_4.svg} \hspace*{-1.5em}
	    \includesvg[width=0.51\linewidth]{../../src/plots/model_0_5.svg}
	    \caption{Six iterations of variance-based active learning on the Hebbal function \cite{hebbal2019bayesian} using a Gaussian process surrogate model with a zero mean function and a rational quadratic covariance function.}
	    \label{fig:sdoe_diagram}
	    \vspace*{-1em}
	\end{figure}
	
	% Why is it hard? Why do naive approaches fail?
	While it is easy to define an approach for sequential design of experiments tasks, there are many challenges in achieving good performance in practice. In particular, the experimental designer must choose a surrogate model that can sufficiently captures the variation in the latent (objective) function in unobserved regions of the input space. Moreover, surrogate models typically require prior knowledge about the behavior of the latent function that may be unavailable. Even more challenging is defining the sequential sampling (active learning) approach so that the surrogate model quickly converges to the latent function. It is important to avoid over-sampling areas of the input space in which there is little variation and also to avoid under-sampling areas of the input space where there is significant nonlinearity.
	
	% What are the key components of my approach and results? Also include any specific limitations. % Summary of the major contributions in bullet form, mentioning in which sections they can be found. This material doubles as an outline of the rest of the paper, saving space and eliminating redundancy.
	In this work, we focus our attention on examining the choice of surrogate model, the incorporation of prior information in the surrogate model, and the choice of active learning approach. We examine Gaussian process (GP) surrogates and neural network (NN) surrogates due to the expressivity of these models. Additionally, we study the effects of a variety of covariance (kernel) functions for GP surrogates and the effects of two activation functions for NN surrogates. We initialize a sample set using a face-centered central composite design (CCD) along with a single random sample and then perform active learning on a series of one-dimensional test functions. We explore five sampling approaches, including random sampling, Sobol sequence sampling, Halton sequence sampling, variance-based sampling, and local linear approximation (LOLA) sampling.
	
    In \Cref{sec:background}, we describe several surrogate models and active learning approaches and their related work. We then elaborate on how these are used in our experiments in \Cref{sec:proposed-approach}. We present the results of several active learning approaches across a variety of surrogate models and test functions in \Cref{sec:experiments-results} and discuss the key findings as they relate to the selection of surrogate models, incorporation of prior information, and choice of active learning approaches in \Cref{sec:conclusion}. In \Cref{sec:future-work} we describe some potential research directions for future work.
	
	\section{Background}
	\label{sec:background}
	
	The design of experiments is a natural part of every experimental domain and finds use across many disciplines. As a result, a wide variety of approaches to the design of experiments has emerged. The groups can be broadly separated by their choice of sample selection scheme and the choice of surrogate model.
	
	\subsection{Sample Selection Schemes}
	
	\subsubsection{Static Sampling Techniques}
	Static sampling techniques are essentially one-shot sampling schemes. The most widely known is the full factorial sampling scheme, which gives a dense grid over the input space. Full factorial sampling requires a number of samples that is exponential with the number of dimensions, which can be prohibitive. Another approach called fractional factorial sampling seeks to reduce the sample complexity \cite{george2005statistics}. 
	
	Due to the exponential sample complexity of dense factorial sampling plans, sparse sampling plans were developed with better sample complexities. Uniform projection sampling plans sparsely generate samples over a uniform grid such that the projections of the sample set is uniform along each dimension \cite{Kochenderfer2019AlgorithmsOptimization}. While uniform projection plans can be useful, they can sometimes fill the input space poorly (consider a diagonal in two-dimensional grid). To improve on this, some low-discrepancy (space-filling) quasi-random sequences, such as the Sobol sequence and the Halton sequence serve as useful alternatives \cite{Sobol1967OnIntegrals,Halton1964AlgorithmSequence}. Sample sequences from second-order (two-dimensional) Sobol and Halton sequences are shown in \Cref{fig:sobol_halton}.
	
	\begin{figure}[htbp]
	    \centering
	    \includesvg[width=0.49\linewidth]{plots/Sobol_sequence_2D.svg}
	    \includesvg[width=0.49\linewidth]{plots/Halton_sequence_2D.svg}
	    \caption{Plots of the first 256 samples of the 2,3 Sobol sequence (left) and the 2,3 Halton sequence (right) \cite{SobolSequence,HaltonSequence}.}
	    \label{fig:sobol_halton}
	\end{figure}
	
    \subsubsection{Adaptive Sampling Techniques}	
    Also called active learning techniques, adaptive sampling techniques identify the next sample based on a metric computed over the surrogate model. For Gaussian process surrogates with predicted mean $\hat{\mu}(\boldsymbol{x})$ and predicted variance $\hat{\sigma}^2({\boldsymbol{x}})$, one convenient technique is variance-based active learning, where the next sample is identified as the sample location with the largest predicted variance.
    \begin{equation}
        \boldsymbol{x}^{(m+1)} = \arg\max_{\boldsymbol{x}\in\mathcal{X}} \hat{\sigma}^2(\boldsymbol{x})
    \end{equation}
    
    While the above approach is exclusive to Gaussian processes, many other techniques can be used for non-probabilistic models and probabilistic models alike. Another active learning is the local linear approximation (LOLA) sampling \cite{article}. We discuss only the one-dimensional case and invite the interested reader to review the higher-dimensional cases in the listed reference. In essence, LOLA identifies samples that would have the largest error between the midpoint of a linear approximation generated using neighboring samples and the surrogate model's prediction at that midpoint. Assuming a sorted sample set where $\boldsymbol{\bar{x}}^{(i,i+1)} = (\boldsymbol{x}^{(i)} + \boldsymbol{x}^{(i+1)}) / 2$, and $\bar{y}^{(i,i+1)} = (y^{(i)} + y^{(i+1)})/2$ the next sample is given by:
    \begin{equation}
         \boldsymbol{x}^{(m+1)} = \arg \max_{\boldsymbol{\bar{x}}^{(i,i+1)}} \left| \bar{y}^{(i,i+1)} - \hat{f}\left(\boldsymbol{\bar{x}}^{(i,i+1)}\right) \right| \quad i \in {1, ..., m-1}
    \end{equation}
    Since the surrogate model is in error from a linear approximation, LOLA guides the identification of samples toward predicted nonlinear regions. There are some slight issues with aliasing when the predicted model is close to the linear approximation at the midpoint, which we observe and discuss later in \Cref{sec:experiments-results}.
    
    Some other techniques for active learning involve using $k$-fold or leave-one-out cross-validation (KFCV, LOOCV) where samples are selected in the neighborhood of points in the fold with large cross-validation error \cite{Kaminsky2018AdaptiveSurfaces} and other techniques that take a Lipschitzian approach \cite{Lovison2010AdaptiveMetamodeling}.

    \subsection{Surrogate Models}
    
    There are many choices for surrogate models, though some important classes include linear functions, rational functions \cite{Hendrickx2005SequentialMetamodelling}, radial basis functions (RBFs) \cite{Mackman2010InvestigationFunctions}, Gaussian processes (GPs) \cite{Simpson2001SamplingAnalysis,VanBeers2005KrigingOverview}, and artificial neural networks (NNs) \cite{Eason2014AdaptiveNetworks}. Herein, we will discuss Gaussian process and neural network surrogates as these models have demonstrated success in modeling broad classes of functions. A detailed discussion of Gaussian processes in given in \cite{williams2006gaussian}.
	
	\subsubsection{Gaussian Processes (GPs)}

	We start with a dataset of $m$ input-output pairs that are stored in matrices $\mathbf{X} \in \mathbb{R}^{m \times d}$ and $\mathbf{y} \in \mathbb{R}^{m}$ respectively. We then seek to develop an estimator for a latent function $f(\mathbf{x})$ that would generate the outputs $\mathbf{y}$ given the inputs $\mathbf{X}$. We model the data generation process as
	\begin{equation}
	    y^{(i)} = f(\boldsymbol{x}^{(i)}) + \epsilon \qquad \text{where~}\epsilon \sim \mathcal{N}(0, \sigma^2 \boldsymbol{I})
	\end{equation}
	and we model the latent function as
	\begin{equation}
	    f(\boldsymbol{x}) \sim \mathcal{GP}(0, k(\boldsymbol{x}, \boldsymbol{x}'))
	\end{equation}
	\textit{assuming a zero mean function and a specific covariance function $k$}.
	
	Using the properties of Gaussian distributions, the conditional distribution of $\hat{\boldsymbol{y}} \mid \boldsymbol{y}$ can be shown to be Gaussian with the following predicted mean and variance (using $\boldsymbol{K}$ as the kernel matrix between two sets of samples):
	\begin{align}
	    \hat{\mu}(\boldsymbol{x}) &= m(\boldsymbol{x}) + \boldsymbol{K}(\boldsymbol{x}, \boldsymbol{X})\boldsymbol{K}(\boldsymbol{X}, \boldsymbol{X})^{-1}(\boldsymbol{y} - m(\boldsymbol{X})) \\
	    \hat{\sigma}^2(\boldsymbol{x}) &= \boldsymbol{K}(\boldsymbol{x}, \boldsymbol{x}) - \boldsymbol{K}(\boldsymbol{x}, \boldsymbol{X})\boldsymbol{K}(\boldsymbol{X}, \boldsymbol{X})^{-1}\boldsymbol{K}(\boldsymbol{X}, \boldsymbol{x})
	\end{align}
    We can fit Gaussian process parameters (mean function and kernel function parameters) using maximum likelihood estimation (MLE) with gradient ascent. 

    While Gaussian processes tend to work well on small datasets and can give probabilistic estimates of the underlying function, they also require modeling assumptions about the mean and covariance of the underlying function. In the case of the design of experiments, it is worthwhile to consider an approach where we may not have prior information about the behavior of the latent function.
    
    \subsubsection{Neural Networks (NNs)}
    
    Given a similar dataset of $m$ input-output pairs that are stored in matrices $\mathbf{X} \in \mathbb{R}^{m \times d}$ and $\mathbf{y} \in \mathbb{R}^{m}$ respectively, we seek to develop an estimator for a latent function $f(\mathbf{x})$ that would generate the outputs $\mathbf{y}$ given the inputs $\mathbf{X}$. Our estimator will be a single-layer neural network with weights and biases of the $i$th layer as $W^{[i]}$ and $b^{[i]}$ and the activation function $\sigma$:
    \begin{align}
        \hat{y} &= W^{[2]}a^{[1]} + b^{[2]} \\
        a^{[1]} &= \sigma(W^{[1]}x + b^{[1]})
    \end{align}
    We train the neural network by minimizing the mean squared error between the predicted outputs and the true outputs through gradient descent.
    
    In comparison to GPs, neural networks benefit from a minimal need for prior knowledge about the latent function. However, neural networks typically require large datasets to learn suitable features. 
    
	\section{Proposed Approach}
	\label{sec:proposed-approach}
	
	We explore several active learning approaches on various test functions, surrogate models, initial sample schemes, and sequential active learning approaches.
	
	\subsection{Test Functions}
	
	\begin{figure*}[htbp]
	    \centering
	    \includesvg[width=0.9\linewidth]{../../src/plots/test_functions.svg}
	    \caption{The six test functions plotted over their domains (\Cref{tab:test_fns}).}
	    \label{fig:test_fns}
	\end{figure*}
	
	\begin{table*}[htbp]
    \renewcommand{\arraystretch}{1.6}
    \centering
    \caption{Test Functions}
    \label{tab:test_fns}
        \begin{tabular}{lll}
        \toprule 
        \bfseries Name & \bfseries Function & \bfseries Domain \\ \midrule
	    \textsc{Hebbal} & $f(x) = -0.5 \sin\left(40 (x-0.85)^4\right) \cos(2(x-0.95)) + 0.5(x-0.9) + 1$ & $x \in [0, 1]$ \\
	    \textsc{Problem15} & $f(x) = (x^2 - 5x + 6) / (x^2 + 1) $ & $x \in [-5, 5]$ \\
	    \textsc{Problem20} & $f(x) = -(x - \sin(x))\exp(-x^2) $ & $x \in [-5, 5]$ \\
	    \textsc{Sinc} & $f(x) = \sinc(x) $ & $x \in [-5, 5]$ \\
	    \textsc{SincShifted} & $f(x) = \sinc(x) $ & $x \in [-5, 15]$ \\
	    \textsc{ZeroMeanStep} & $f(x) = (x > 0) - 0.5 $ & $x \in [-2, 2]$ \\
	    \bottomrule
	    \end{tabular}
	\end{table*}
	
	We consider six different one-dimensional test functions (\Cref{tab:test_fns}) due to the simplicity of implementing active learning approaches in one dimension. Future work could include extending some of the active learning methods to two or more dimensions.
	
	The first function is the \textsc{Hebbal} function, which displays heteroscedastic behavior that would be difficult for a GP to learn. The \textsc{Hebbal} function is a modified version of the Xiong function that was studied by Hebbal et al. \cite{hebbal2019bayesian}. The second and third problems (\textsc{Problem15} and \textsc{Problem20}) are two homoscedastic optimization test functions with smooth behavior \cite{1-DFunctions}. Finally, the last three functions are test function we chose to study due to the contrast they provide with the other functions. The \textsc{Sinc} function is highly oscillatory, with a large peak near the origin, and an altered domain (\textsc{SincShifted}) could change the perceived covariance. The \textsc{Step} function is purely pathological, but worth studying due to the discontinuity. 
	
	\subsection{Surrogate Models}
	
	We examine both Gaussian process (GP) and neural network (NN) surrogate models due to their expressivity. The GPs are constructed, fit, and evaluated using scikit-learn \cite{Scikit-learn:Python} while the NNs are constructed, trained, and evaluated using Keras \cite{Keras:API}.
	
	\subsubsection{Gaussian Process Hyperparameters}
	
	\begin{table*}[htbp]
    \renewcommand{\arraystretch}{2.2}
    \centering
    \caption{Gaussian Process Kernels}
    \label{tab:gp_kernels}
        \begin{tabular}{lll}
        \toprule 
        \bfseries Kernel & \bfseries Equation & \bfseries Parameters \\ \midrule
	    Constant &  $k(\mathbf{x}, \mathbf{x}') = \sigma_0^2$ & $\sigma_0^2 \in [0, \infty)$ \\
	    Dot product & $k(\mathbf{x}, \mathbf{x}') = \sigma_0^2 + \mathbf{x}^T \mathbf{x}'$ & $\sigma_0^2 \in [0, \infty)$ \\
	    Squared exponential (RBF) & $k(\mathbf{x}, \mathbf{x}') = \exp \left( -\frac{||\mathbf{x} - \mathbf{x}'||_2^2}{2\ell^2} \right)$ & $\ell \in (0, \infty)$ \\
	    Rational quadratic & $k(\mathbf{x}, \mathbf{x}') = \left(1+\frac{||\mathbf{x} - \mathbf{x}'||_2^2}{2\alpha \ell^2}\right)^{-\alpha}$ & $\ell, \alpha \in (0, \infty)$ \\
	    Mat\'ern & $k(\mathbf{x}, \mathbf{x}') = \frac{1}{\Gamma(\nu)2^{\nu-1}}\left(\frac{\sqrt{2\nu}}{l} ||\mathbf{x} - \mathbf{x}'||_2^2 \right)^\nu K_\nu\left(\frac{\sqrt{2\nu}}{l} ||\mathbf{x} - \mathbf{x}'||_2^2 \right)$ & $\nu \in (0, \infty)$ \\ \bottomrule
	    \end{tabular}
	\end{table*}
	
	Using the dataset of input-output pairs, we fit a Gaussian process by minimizing the log-marginal-likelihood of the mean function and covariance function parameters given the dataset. For the minimization, we use the limited-memory Broyden-Fletcher-Goldfarb-Shanno with bound constraints optimizer (\textsc{L-BFGS-B}).
	
	While GPs are non-parametric estimators, they require specifying a prior over the GP, given by the mean function $m(\boldsymbol{x})$ and the covariance function $k(\boldsymbol{x}, \boldsymbol{x}')$. The choice of mean function and covariance function is non-trivial and can greatly influence the resulting distribution over the latent function. Typically, prior knowledge about behavior of the latent function over the design space is utilized in constructing the mean function and covariance function.
	
	For convenience, we assume a zero mean function. In our experiments across all active learning methods, we examine a variety of standard, non-composed, isotropic kernel functions. In particular, we examine the constant, dot product, squared exponential (RBF), rational quadratic, and Mat\'ern ($\nu = \{\tfrac{1}{2}, \tfrac{3}{2}, \tfrac{5}{2}\}$) kernel functions, which are listed in \Cref{tab:gp_kernels}, where $\Gamma$ is the gamma function and $K_\nu$ is the Bessel function of order $\nu$. 
	
	\subsubsection{Neural Network Architecture \& Hyperparameters}
	
	In the language of machine learning, our problem can be framed as a supervised learning problem. Given our dataset, we train a neural network to minimize the mean-squared-error (MSE) of the input-output pairs. We use the adaptive moment estimation (\textsc{Adam}) algorithm to minimize the loss. 
	
	The neural network and optimizer hyperparameters are listed in \Cref{tab:nn_hyperparams}. We note that the choice of a single-layer shallow neural network rather than a deep neural network is to achieve a fair comparison with the shallow GP. Despite this limitation, it was shown in the universal approximation theorem \cite{Cybenko1989ApproximationFunction} that a single-layer feed-forward neural network with a finite number of hidden units can approximate continuous functions with up to an arbitrary degree of precision. We select 64 hidden units since it appears to give good function approximations. We also examine both the rectified linear unit (ReLU) \cite{Nair2010RectifiedMachines} and scaled exponential linear unit (SELU) \cite{Klambauer2017Self-normalizingNetworks} activation functions, depicted in \Cref{fig:activ_fns} and given in the following equations, where $\alpha$ and $\lambda$ are scaling parameters for the SELU activation function.
	
	\begin{align*}
	    \text{ReLU}(x) &= \max(0, x) \\
	    \text{SELU}(\alpha, x) &= \lambda 
	        \begin{cases}
                \alpha (e^x - 1) & \text{if $x < 0$} \\
                x & \text{if $x \ge 0$}
            \end{cases}   
	\end{align*}
	
	\begin{table}[htbp]
    \renewcommand{\arraystretch}{1.0}
    \centering
    \caption{Neural Network and Optimizer Hyperparameters}
    \label{tab:nn_hyperparams}
        \begin{tabular}{lc}
        \toprule 
        \bfseries Hyperparameter & \bfseries Value \\ \midrule
	    Number of layers & 1 \\
	    Number of hidden units & 64 \\ 
	    Hidden layer activation fns., $\sigma(z)$ & ReLU \& SELU \\
	    First moment decay parameter, $\beta_1$ & 0.9 \\
	    Second moment decay parameter, $\beta_2$ & 0.999 \\
	    Stability constant, $\epsilon$ & 1E-7 \\ 
	    Initial learning rate, $\alpha_0$ & 1E-2 \\ 
	    Learning rate reduction factor & 0.5 \\
	    Learning rate reduction criterion & 500 iters. no impr. \\
	    Minimum learning rate & 1E-5 \\
	    Early stopping criterion & 1500 iters. no impr. $\ge$ 1E-6 \\ 
	    Maximum number of epochs & 10000 \\ \bottomrule
	    \end{tabular}
	\end{table}
	
	\begin{figure}[htbp]
	    \centering
	    \vspace*{-1em}
	    \includesvg[width=\linewidth]{../../src/plots/activation_functions.svg}
	    \vspace*{-2em}
	    \caption{The rectified linear unit (ReLU) and scaled exponential linear unit (SELU) activation functions (SELU: $\alpha = 1$, $\lambda =1$). While ReLU is separably linear with straightforward gradients for positive and negative inputs, SELU is nonlinear and has a more complex gradient for negative inputs.}
	    \label{fig:activ_fns}
	\end{figure}
	
	Since our neural network has only a single hidden layer, we simply want to investigate the difference between a piecewise linear activation function and nonlinear activation function. Providing pure nonlinearity (rather than piecewise linearity) will enable the single-layer neural network to generate a much smoother approximation that varies between training examples when the dataset is small. This variation can be exploited to improve learning and is discussed further in the results. Using both the rectified linear unit (ReLU) and scaled exponential linear unit (SELU) activation functions, we train for a maximum of 10,000 epochs with an initial learning rate of 0.01, that is reduced by a factor of 0.5 if there are no improvements for 500 epochs. Additionally, we stop early if there is no improvement greater that 1E-6 for 1500 epochs.
	
	\subsection{Initial Sampling Scheme}
	
	Typically, using a random initialization would make sense, however, this tends to lead to excessive exploration near the boundaries of the the design space. To mitigate this, we make use of the face-centered central composite design (CCD, or CCF) sampling scheme, which is essentially a full factorial grid over the design space, but with $m=3$ samples along each dimension \cite{CentralCCD}. Once we have obtained these samples, we also add a single random sample to seed the learning of the metamodel. While adding a single random sample is not necessary, it helps us quantify the robustness of active learning approaches.
	
	\subsection{Sequential Sampling Methods}
	
	As mentioned in \Cref{sec:background}, there are many sequential sampling approaches that we can use. We will test five different sampling approaches including random sampling, Sobol sequence sampling, Halton sequence sampling, variance-based active learning, and local linear approximation (LOLA) active sampling. Neural networks will not be tested on variance-based active learning due to the lack of a variance estimator.
	    
	\section{Experiments \& Results}
	\label{sec:experiments-results}
	
	Computing integrated square error by using 1001 equally spaced points along the interval.
	
	\begin{figure}[ht]
		\centering
		\includesvg[width=\linewidth]{../../src/plots/sinc/gp_variance/kernel_comparison_ise_iter_50_50.svg}
		\caption{Ablation study of various non-composed covariance functions (kernels) for a Gaussian process with $k=50$ additional samples sequentially selected using variance-based active learning. $n=50$ initial sample schemes using a baseline central composite design (CCD) with a single random sample.}
	\end{figure}
	
	
	Active learning should be as non-parametric as possible since it should adapt to an unknown objective.
	
	\section{Conclusion}
	\label{sec:conclusion}
	
	We have shown that GPs are typically more robust surrogate models than NNs for one-dimensional sequential design of experiments problems. While GPs can achieve much lower integrated squared error (ISE) than NNs, this is largely dependent on the choice of kernel function, which may not be able to be selected \textit{a priori}.
	
	\section{Future Work}
	\label{sec:future-work}
	
	In the future, it would be interesting to implement the local linear approximation (LOLA) and $k$-fold cross validation (KFCV) active learning methods in multiple dimensions to begin to look at the scalability of GPs and NNs. Another important direction for future work is also the extension of the experiments to deep Gaussian processes (DGPs) and deep neural networks (DNNs) \cite{Damianou2013DeepProcesses}. The expressivity and principled composability of DGPs and DNNs may lead to significant improvements over their shallow counterparts in cases when there are sufficiently large datasets. We foresee great improvement particularly in high-dimensional spaces, where large dataset will be necessary in order to efficiently construct a surrogate model that can capture the underlying variation in the latent function. An interesting application of ongoing research could be to implement and examine the performance of the neural network Gaussian process (NNGP), which uses a neural network kernel function to model the prediction process of a Bayesian neural network (BNN).
	
	\bibliographystyle{IEEEtran}
	\bibliography{final}
	
	
\end{document}
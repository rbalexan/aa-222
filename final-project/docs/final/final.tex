\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{svg}

\usepackage{booktabs} %@{}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage[per-mode=symbol,detect-all]{siunitx}
\usepackage{hyperref}
\usepackage{cleveref} %\Cref{} vs. \cref{}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{mathabx} % for \bigtimes

\DeclareMathOperator{\sinc}{sinc}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
	
	
	\title{\LARGE \textbf{Active Learning for Efficiently Constructing Surrogate Models}}
	
	
	\author{\IEEEauthorblockN{  Ross Alexander}
		\IEEEauthorblockA{\textit{  Department of Aeronautics and Astronautics} \\
			\textit{                    Stanford University} \\
			Stanford, CA 94305 \\
			rbalexan@stanford.edu}} % or ORCID
	
	
	\maketitle
	
	\begin{abstract}
		
	\end{abstract}
	
	% \begin{IEEEkeywords}
	% component, formatting, style, styling, insert
	% \end{IEEEkeywords}
	
	\section{Introduction}
	\label{sec:introduction}
	
	% Overview of topic sentences
	
	% What is the problem?  
	The design of experiments is a challenging and complex task that requires defining a sampling scheme that can efficiently cover the specified design space and a global metamodel that can capture the variation in the underlying process. While it is often simple to define static sampling plans, such as a full factorial sampling plan or a random sampling plan, these sampling plans can suffer from a lack of scalability or a lack of efficient coverage of the design space. It is important to avoid over-sampling areas of the design space in which there is little variation and also to avoid under-sampling areas of the design space where there is significant nonlinearity. Apart from the sampling scheme, it is critical to select a metamodel that is sufficiently expressive. This can range from a simple linear regression to a neural network.
	
	% Why is it interesting and important?
	% Why is it hard? Why do naive approaches fail?
	In our case, rather than optimizing an objective function, we want to optimize the global metamodel.
	Sequential design of experiments requires both an expressive metamodel and a
	
	% Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
	
	% What are the key components of my approach and results? Also include any specific limitations.
	
	% Summary of the major contributions in bullet form, mentioning in which sections they can be found. This material doubles as an outline of the rest of the paper, saving space and eliminating redundancy.
	
	
	\section{Related Work}
	\label{sec:related-work}
	
	Full factorial, fractional factorial, central composite design, random sampling, uniform projection, stratified sampling, space filling metrics
	
	use of active learning techniques: variance based, LOLA, KFCV, LOOCV, other approaches
	
	disciplines in which activel learning can be used
	
	models that can be used: GPs, NNs, DGPs, DNNs, least-squares linear regression, or feature-mapped LSLR
	
	\section{Background}
	\label{sec:background}
	
	Gaussian processes are suitable for modelling small datasets where some prior knowledge of the generative process exists. GPs do require assumptions about the functional form of the underlying response. GPs do not scale well in terms of dimensionality. They may provide well calibrated uncertainty output.

    Neural networks are, on the other hand, more suitable for large and very large data sets where little knowledge about the underlying process or suitable features exist. NNs scale well. Work is being done to enable neural networks to output calibrated uncertainty estimates.
    
    models that have demonstrated success in modeling functions.
    
    \subsection{Metamodels}
    
    \subsubsection{Least-Squares Predictors}
    
    Traditional methods involving least-squares regression can be used to fit a model of the objective function using a dataset $\mathcal{D} = \{ (\boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)}) \}_{i=1}^m$. We model the predictor $\hat f$ as the inner product of the sample $\boldsymbol x \in \mathbb{R}^d$ with the parameter vector $\boldsymbol \theta \in \mathbb{R}^d$:
    \begin{equation}
        \hat f(\boldsymbol x) = \boldsymbol \theta ^T \boldsymbol x
    \end{equation}
	Using a sum-squared-error loss function, the minimizer can be obtained in closed-form using the Moore-Penrose pseudoinverse.
	\begin{equation}
	    \boldsymbol{\theta} = \boldsymbol{X}^+ \boldsymbol{y}
	\end{equation}
	where $\boldsymbol X \in \mathbb{R}^{m \times d}$ is the sample matrix (with samples transposed and then concatenated vertically) and where $\boldsymbol y \in \mathbb{R}^m$ is the output vector (with outputs concatenated vertically). 

	While linear features can be useful, in practice, the variation in the underlying process can be nonlinear. Using a feature map $\phi: \mathbb{R}^d \mapsto \mathbb{R}^p$ that maps a sample from the natural space in $d$ dimensions to the feature space in $p$ dimensions, where $p > d$, we can again perform linear regression, however in the higher-dimensional feature space. Similarly to above, we can define a predictor $\hat f$ as the inner product of the feature-mapped sample $\boldsymbol{\phi(x)} \in \mathbb{R}^p$ with the parameter vector $\boldsymbol{\theta} \in \mathbb{R}^p$:
    \begin{equation}
        \hat f(\boldsymbol{\phi(x)}) = \boldsymbol{\theta}^T \boldsymbol{\phi(x)}
    \end{equation}
    which can be similarly solved in a closed form where $\boldsymbol \Phi \in \mathbb{R}^{m \times p}$ is the transformed sample matrix (with samples transposed and then concatenated vertically).
    \begin{equation}
	    \boldsymbol{\theta} = \boldsymbol{\Phi}^+ \boldsymbol{y}
	\end{equation}
	
	feature-mapped regresstion
	
	% typical GP derivation
	% prior and posterior predictive distribution 
	\begin{equation*}
	GP(m, k)
	\end{equation*}
	
	We hope to compare GPs, DGPs, NNs, and DNNs in the future, but for now we just compare GPs and NNs. Even though we have the Universal Approximation theorem for single-layer NNs, it doesn't say how hard it will be (also activation fns, training, learning rate, optimizer, etc difficulties). GPs have similar issues with kernel choice and composition since there are many possible ways to construct kernels. The MLE of the parameters is a non-convex optimization and so it is possible to be stuck in a local maximum that is not globally optimal
	
	\subsubsection{Gaussian Process Predictors}
	
	We start with a dataset of $m$ input-output pairs that are stored in matrices $\mathbf{X} \in \mathbb{R}^{m \times d_x}$ and $\mathbf{Y} \in \mathbb{R}^{m \times d_y}$ respectively. We then seek to develop an estimator for a latent function $f(\mathbf{x})$ that would generate the outputs $\mathbf{Y}$ given the inputs $\mathbf{X}$.
	
	A Gaussian process (GP) is a \cite{williams2006gaussian}.
	
	\subsubsection{Artificial Neural Network Predictors}
	
	\section{Proposed Approach}
	\label{sec:proposed-approach}
	
	We explore several active learning approaches on various test functions, metamodels, initial sample schemes, and sequential active learning approaches.
	
	\subsection{Test Functions}
	
	We consider six different one-dimensional test functions (\Cref{tab:test_fns}) due to the simplicity of implementing active learning approaches in one dimension. Future work could include extending some of the active learning methods to two or more dimensions.
	
	The first function is the Hebbal function, which displays heteroscedastic behavior that would be difficult for a GP to learn. The Hebbal function is a modified version of the Xiong function that was studied by Hebbal et al. \cite{hebbal2019bayesian}. The second and third problems (Problem15 and Problem20) are two homoscedastic optimization test functions with smooth behavior \cite{1-DFunctions}. Finally, the last three functions are test function we chose to study due to the contrast they provide with the other functions. The $\sinc$ function is highly oscillatory, with a large peak near the origin, and an altered domain could change the perceived covariance. The step function is purely pathological, but worth studying due to the discontinuity. 
	
	\begin{table*}[htbp]
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption{Test Functions}
    \label{tab:test_fns}
        \begin{tabular}{lll}
        \toprule 
        \bfseries Name & \bfseries Function & \bfseries Domain \\ \midrule
	    \textsc{Hebbal} & $f(x) = -0.5 \sin\left(40 (x-0.85)^4\right) \cos(2(x-0.95)) + 0.5(x-0.9) + 1$ & $x \in [0, 1]$ \\
	    \textsc{Problem15} & $f(x) = (x^2 - 5x + 6) / (x^2 + 1) $ & $x \in [-5, 5]$ \\
	    \textsc{Problem20} & $f(x) = -(x - \sin(x))\exp(-x^2) $ & $x \in [-5, 5]$ \\
	    \textsc{Sinc} & $f(x) = \sinc(x) $ & $x \in [-5, 5]$ \\
	    \textsc{SincShifted} & $f(x) = \sinc(x) $ & $x \in [-5, 15]$ \\
	    \textsc{ZeroMeanStep} & $f(x) = (x > 0) - 0.5 $ & $x \in [-2, 2]$ \\
	    \bottomrule
	    \end{tabular}
	\end{table*}
	
	\begin{figure*}[htbp]
	    \centering
	    \includesvg[width=0.9\linewidth]{../../src/plots/test_functions.svg}
	    \caption{The six test functions plotted over their domains (\Cref{tab:test_fns}).}
	    \label{fig:test_fns}
	\end{figure*}
	
	\subsection{Metamodels}
	
	We examine both Gaussian process (GP) and neural network (NN) metamodels due to their expressivity. The GPs are constructed, fit, and evaluated using scikit-learn \cite{Scikit-learn:Python} while the NNs are constructed, trained, and evaluated using Keras \cite{Keras:API}.
	
	\subsubsection{Gaussian Process Hyperparameters}
	
	Using the a dataset of input-output pairs, we fit a Gaussian process by minimizing the log-marginal-likelihood of the mean function and covariance function parameters given the dataset. For the minimization, we use the limited-memory Broyden-Fletcher-Goldfarb-Shanno with bound constraints optimizer (\textsc{L-BFGS-B}).
	
	While GPs are non-parametric estimators, they require specifying a prior over the GP, given by the mean function $\boldsymbol{m}(\boldsymbol{x})$ and the covariance function $k(\boldsymbol{x}, \boldsymbol{x}')$. The choice of mean function and covariance function is non-trivial and can greatly influence the resulting distribution over the latent function. Typically, prior knowledge about behavior of the latent function over the design space is utilized in constructing the mean function and covariance function.
	
	For convenience, we assume a zero mean function. In our experiments across all active learning methods, we examine a variety of standard, non-composed, isotropic kernel functions listed in \Cref{tab:gp_kernels}. In particular, we examine the constant, dot product, squared exponential (RBF), rational quadratic, and Mat\'ern ($\nu = \{\tfrac{1}{2}, \tfrac{3}{2}, \tfrac{5}{2}\}$) kernel functions. 
	
	\begin{table*}[htbp]
    \renewcommand{\arraystretch}{2.2}
    \centering
    \caption{Gaussian Process Kernels}
    \label{tab:gp_kernels}
        \begin{tabular}{lll}
        \toprule 
        \bfseries Kernel & \bfseries Equation & \bfseries Parameters \\ \midrule
	    Constant &  $k(\mathbf{x}, \mathbf{x}') = \sigma_0^2$ & $\sigma_0^2 \in [0, \infty)$ \\
	    Dot product & $k(\mathbf{x}, \mathbf{x}') = \sigma_0^2 + \mathbf{x}^T \mathbf{x}'$ & $\sigma_0^2 \in [0, \infty)$ \\
	    Squared exponential (RBF) & $k(\mathbf{x}, \mathbf{x}') = \exp \left( -\frac{||\mathbf{x} - \mathbf{x}'||_2^2}{2\ell^2} \right)$ & $\ell \in (0, \infty)$ \\
	    Rational quadratic & $k(\mathbf{x}, \mathbf{x}') = \left(1+\frac{||\mathbf{x} - \mathbf{x}'||_2^2}{2\alpha \ell^2}\right)^{-\alpha}$ & $\ell, \alpha \in (0, \infty)$ \\
	    Mat\'ern & $k(\mathbf{x}, \mathbf{x}') = \frac{1}{\Gamma(\nu)2^{\nu-1}}\left(\frac{\sqrt{2\nu}}{l} ||\mathbf{x} - \mathbf{x}'||_2^2 \right)^\nu K_\nu\left(\frac{\sqrt{2\nu}}{l} ||\mathbf{x} - \mathbf{x}'||_2^2 \right)$ & $\nu \in (0, \infty)$ \\ \bottomrule
	    \end{tabular}
	\end{table*}
	
	\subsubsection{Neural Network Architecture \& Hyperparameters}
	
	In the language of machine learning, our problem can be framed as a supervised learning problem. Given our dataset, we train a neural network to minimize the mean-squared-error (MSE) of the input-output pairs. We use the adaptive moment estimation (\textsc{Adam}) algorithm to minimize the loss. 
	
	The neural network and optimizer hyperparameters are listed in \Cref{tab:nn_hyperparams}. We note that the choice of a single-layer shallow neural network is to achieve a fair comparison with the shallow GP. Despite this limitations, it was shown in the universal approximation theorem \cite{Cybenko1989ApproximationFunction} that a single-layer feed-forward neural network with a finite number of hidden units can approximate continuous functions with up to an arbitrary degree of precision. We select 64 hidden units since it appears to give good function approximations. We also examine both the rectified linear unit (ReLU) \cite{Nair2010RectifiedMachines} and scaled exponential linear unit (SELU) \cite{Klambauer2017Self-normalizingNetworks} activation functions, depicted in \Cref{fig:activ_fns} and given in the following equations, where $\alpha$ and $\lambda$ are scaling parameters for the SELU activation function.
	\begin{align*}
	    \text{ReLU}(x) &= \max(0, x) \\
	    \text{SELU}(\alpha, x) &= \lambda 
	        \begin{cases}
                \alpha (e^x - 1) & \text{if $x < 0$} \\
                x & \text{if $x \ge 0$}
            \end{cases}   
	\end{align*}
	Since our neural network has only a single hidden layer, we simply want to investigate the difference between a piecewise linear activation function and nonlinear activation function. Providing pure nonlinearity (rather than piecewise linearity) will enable the single-layer neural network to generate a much smoother approximation that varies between training examples when the dataset is small. This variation can be exploited to improve learning and is discussed further in the results. Using both the rectified linear unit (ReLU) and scaled exponential linear unit (SELU) activation functions, we train for a maximum of 10,000 epochs with an initial learning rate of 0.01, that is reduced by a factor of 0.5 if there are no improvements for 500 epochs. Additionally, we stop early if there is no improvement greater that 1E-6 for 1500 epochs.
	
	\begin{table}[htbp]
    \renewcommand{\arraystretch}{1.0}
    \centering
    \caption{Neural Network and Optimizer Hyperparameters}
    \label{tab:nn_hyperparams}
        \begin{tabular}{lc}
        \toprule 
        \bfseries Hyperparameter & \bfseries Value \\ \midrule
	    Number of layers & 1 \\
	    Number of hidden units & 64 \\ 
	    Hidden layer activation fns., $\sigma(z)$ & ReLU \& SELU \\
	    First moment decay parameter, $\beta_1$ & 0.9 \\
	    Second moment decay parameter, $\beta_2$ & 0.999 \\
	    Stability constant, $\epsilon$ & 1E-7 \\ 
	    Initial learning rate, $\alpha_0$ & 1E-2 \\ 
	    Learning rate reduction factor & 0.5 \\
	    Learning rate reduction criterion & 500 iters. no impr. \\
	    Minimum learning rate & 1E-5 \\
	    Early stopping criterion & 1500 iters. no impr. $\ge$ 1E-6 \\ 
	    Maximum number of epochs & 10000 \\ \bottomrule
	    \end{tabular}
	\end{table}
	
	\begin{figure}[htbp]
	    \centering
	    \vspace*{-1em}
	    \includesvg[width=\linewidth]{../../src/plots/activation_functions.svg}
	    \vspace*{-2em}
	    \caption{The rectified linear unit (ReLU) and scaled exponential linear unit (SELU) activation functions (SELU: $\alpha = 1$, $\lambda =1$). While ReLU is separably linear with straightforward gradients for positive and negative inputs, SELU is nonlinear and has a more complex gradient for negative inputs.}
	    \label{fig:activ_fns}
	\end{figure}
	
	\subsection{Initial Sampling Scheme}
	
	Typically, using a random initialization would make sense, however, this tends to lead to excessive exploration near the boundaries of the the design space. To mitigate this, we make use of the face-centered central composite design (CCD, or CCF) sampling scheme, which is essentially a full factorial grid over the design space, but with $m=3$ samples along each dimension \cite{CentralCCD}. Once we have obtained these samples, we also add a single random sample to seed the learning of the metamodel. While adding a single random sample is not necessary, it helps us quantify the robustness of active learning approaches.
	
    \subsection{Adaptive Sampling Schemes}
	
	We don't know the underlying model and we actually aer trying to identify the interesting points of a model just based on an approximate model that gets increasingly more accurate.

	
	\subsubsection{Random Sequence Sampling}
	
	\subsubsection{Low-Discrepancy Sequence Sampling}
	
	van der Corput / Halton & Sobol
	
	Quasi-random sequences are deterministic sequences that appear random, but have underlying structure. 
	
	What does discrepancy represent? \cite{kuipers1974uniform}
	
	The discrepancy of a set of points $X$ is defined in \Cref{eqn:discrepancy}, which represents the supremum of the absolute difference between the fraction of points in $X$ that are also within hyper-rectangle $\mathcal{H}$ and the Lebesgue measure of the hyper-rectangle $\lambda(\mathcal{H})$.
	
	\begin{equation}
	    \label{eqn:discrepancy}
	    d(X) = \sup_\mathcal{H} \left| \frac{|X \cap \mathcal{H}|}{|X|} - \lambda(\mathcal{H}) \right|
	\end{equation}

	Since computing the discrepancy can be quite difficult and minimizing the discrepancy  can be even more challenging, we often settle for low-discrepancy sequences. We seek a low-discrepancy sequence since it will be the most space-filling an thus the error in modeling the underlying function will converge as quickly as possible compared to purely random sequence. There are a variety of quasi-random sequences that are designed to be low-discrepancy - van der Corput, Halton, Sobol, additive recurrence.

	\subsubsection{Variance-Based Active Learning}
	
	\subsubsection{Local Linear Approximation-Based Active Learning}
	Local linear approximation (since deviation from linear implies nonlinearity and is an interesting feature that is worth capturing)
	    
	\section{Experiments \& Results}
	\label{sec:experiments-results}
	
	Computing integrated square error by using 1001 equally spaced points along the interval.
	
	\begin{figure}[ht]
		\centering
		\includesvg[width=\linewidth]{../../src/plots/sinc/gp_variance/kernel_comparison_ise_iter_50_50.svg}
		\caption{Ablation study of various non-composed covariance functions (kernels) for a Gaussian process with $k=50$ additional samples sequentially selected using variance-based active learning. $n=50$ initial sample schemes using a baseline central composite design (CCD) with a single random sample.}
	\end{figure}
	
	
	Active learning should be as non-parametric as possible since it should adapt to an unknown objective.
	
	\section{Conclusion}
	\label{sec:conclusion}
	
	
	\section{Future Work}
	\label{sec:future-work}
	
	% need citations
	In the future, it would be interesting to explore extending the local linear approximation (LOLA) active learning method to multiple dimensions. It would also be interesting to examine a $k$-fold cross validation (KFCV) active learning method.
	
	Another areas of focus for future work could be on implementing and examining the performance of the neural network Gaussian process (NNGP), which uses a neural network kernel function to model the prediction process of a Bayesian neural network (BNN).
	
	An important direction for future work is the extension of the experiments to deep Gaussian processes (DGPs) and deep neural networks (DNNs). The expressivity and principled composability of DGPs and DNNs may lead to significant improvements over their shallow counterparts in cases when there are sufficiently large datasets. We foresee great improvement particularly in high-dimensional spaces, where large dataset will be necessary in order to efficiently construct a surrogate model that can capture the underlying variation in the latent function.
	
	
	\bibliographystyle{IEEEtran}
	\bibliography{final}
	
	
\end{document}